# yaml-language-server: $schema=bundle_config_schema.json
# This is a Databricks asset bundle definition for my_project2.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.
bundle:
  name: my_project2

# include:
#   - resources/*.yml

targets:
  # The 'dev' target, used for development purposes.
  # Whenever a developer deploys using 'dev', they get their own copy.
  dev:
    # We use 'mode: development' to make sure everything deployed to this target gets a prefix
    # like '[dev my_user_name]'. Setting this mode also disables any schedules and
    # automatic triggers for jobs and enables the 'development' mode for Delta Live Tables pipelines.
    mode: development
    default: true
    workspace:
      host: https://adb-1209901303365609.9.azuredatabricks.net
      root_path: /Users/${workspace.current_user.userName}/.bundle/${bundle.target}/${bundle.name}    

    resources:
      jobs:
        my_project2_job:
          name: my_project2_job_${bundle.target}

          tasks:
            - task_key: notebook_task
              # job_cluster_key: job_cluster
              existing_cluster_id: 0729-134934-wrndqqvh
              notebook_task:
                # notebook_path: ../src/notebook.ipynb
                notebook_path: /Users/${workspace.current_user.userName}/.bundle/${bundle.target}/${bundle.name}/files/src/notebook              
                base_parameters: 
                  Environment: ${bundle.target}
                  top_k: "6"
            - task_key: unit_test_task
              depends_on:
                - task_key: notebook_task
              # job_cluster_key: job_cluster
              existing_cluster_id: 0729-134934-wrndqqvh
              notebook_task:
                notebook_path: /Users/${workspace.current_user.userName}/.bundle/${bundle.target}/${bundle.name}/files/src/run_unit_tests

  # Optionally, there could be a 'staging' target here.
  # (See Databricks docs on CI/CD at https://docs.databricks.com/dev-tools/bundles/index.html.)
  #
  staging:
    mode: production
    workspace:
      host: https://adb-1209901303365609.9.azuredatabricks.net
      root_path: /Shared/.bundle/Staging/${bundle.name}
    run_as:
      # This runs as charles.chen@databricks.com in production. Alternatively,
      # a service principal could be used here using service_principal_name
      # (see Databricks documentation).
      user_name: charles.chen@databricks.com

    resources:
      jobs:
        my_project2_job:
          name: my_project2_job_${bundle.target}

          schedule:
            quartz_cron_expression: '00 30 08 * * ?'
            timezone_id: America/New_York

          email_notifications:
            on_failure:
              - charles.chen@databricks.com

          tasks:
            - task_key: notebook_task
              job_cluster_key: job_cluster
              notebook_task:
                # notebook_path: ../src/notebook.ipynb
                notebook_path: /Shared/.bundle/Staging/my_project2/files/src/notebook
                base_parameters: 
                  Environment: ${bundle.target}
                  top_k: "7"

          job_clusters:
            - job_cluster_key: job_cluster
              new_cluster:
                spark_version: 13.3.x-scala2.12
                node_type_id: Standard_D3_v2
                autoscale:
                    min_workers: 1
                    max_workers: 4

  # The 'prod' target, used for production deployment.
  prod:
    # For production deployments, we only have a single copy, so we override the
    # workspace.root_path default of
    # /Users/${workspace.current_user.userName}/.bundle/${bundle.target}/${bundle.name}
    # to a path that is not specific to the current user.
    mode: production
    workspace:
      host: https://adb-1209901303365609.9.azuredatabricks.net
      root_path: /Shared/.bundle/Prod/${bundle.name}
    run_as:
      # This runs as charles.chen@databricks.com in production. Alternatively,
      # a service principal could be used here using service_principal_name
      # (see Databricks documentation).
      user_name: charles.chen@databricks.com
    
    resources:
      jobs:
        my_project2_job:
          name: my_project2_job_${bundle.target}

          schedule:
            quartz_cron_expression: '00 45 08 * * ?'
            timezone_id: America/New_York

          email_notifications:
            on_failure:
              - charles.chen@databricks.com

          tasks:
            - task_key: notebook_task
              job_cluster_key: job_cluster
              notebook_task:
                # notebook_path: ../src/notebook.ipynb        
                notebook_path: /Shared/.bundle/Prod/my_project2/files/src/notebook
                base_parameters: 
                  Environment: ${bundle.target}
                  top_k: "8"

          job_clusters:
            - job_cluster_key: job_cluster
              new_cluster:
                spark_version: 13.3.x-scala2.12
                node_type_id: Standard_D3_v2
                autoscale:
                    min_workers: 1
                    max_workers: 4
